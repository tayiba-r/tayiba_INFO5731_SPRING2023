{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tayiba-r/tayiba_INFO5731_SPRING2023/blob/main/exercises/In_class_exercise_03_02282023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm61vFPtEu4E"
      },
      "source": [
        "## The third In-class-exercise (2/28/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTCDQCIjEu4I"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOS7YZYZEu4K"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "jG4kUdbuEu4L",
        "outputId": "f270c7c7-5f02-42a4-f5ac-da8a14e3c834"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\nOne interesting text classification task is to predict the category of a news article based on its content. \\nThis can be useful for news websites to automatically categorize their articles or for news aggregators to sort articles into different topics. \\nFor example, given an article about sports, the model should predict that it belongs to the \"Sports\" category.\\n\\nBag of words: This feature represents the frequency of each word in the article. \\nThe model can learn to associate certain words with specific categories. \\n\\nNamed entities: This feature extracts the named entities from the article, such as people, organizations, and locations. \\nThe model can learn to associate certain entities with specific categories.\\n\\nPart-of-speech tags: This feature represents the grammatical category of each word, such as noun, verb, or adjective. \\nThe model can learn to associate certain parts of speech with specific categories. \\n\\nSentiment: This feature represents the sentiment of the article, such as positive, negative, or neutral. \\nThe model can learn to associate certain sentiment with specific categories. \\n\\nReadability: This feature represents the readability of the article, such as the average length of sentences and the complexity of vocabulary. \\nThe model can learn to associate certain levels of readability with specific categories.\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "One interesting text classification task is to predict the category of a news article based on its content. \n",
        "This can be useful for news websites to automatically categorize their articles or for news aggregators to sort articles into different topics. \n",
        "For example, given an article about sports, the model should predict that it belongs to the \"Sports\" category.\n",
        "\n",
        "Bag of words: This feature represents the frequency of each word in the article. \n",
        "The model can learn to associate certain words with specific categories. \n",
        "\n",
        "Named entities: This feature extracts the named entities from the article, such as people, organizations, and locations. \n",
        "The model can learn to associate certain entities with specific categories.\n",
        "\n",
        "Part-of-speech tags: This feature represents the grammatical category of each word, such as noun, verb, or adjective. \n",
        "The model can learn to associate certain parts of speech with specific categories. \n",
        "\n",
        "Sentiment: This feature represents the sentiment of the article, such as positive, negative, or neutral. \n",
        "The model can learn to associate certain sentiment with specific categories. \n",
        "\n",
        "Readability: This feature represents the readability of the article, such as the average length of sentences and the complexity of vocabulary. \n",
        "The model can learn to associate certain levels of readability with specific categories.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oL5WZJvEu4N"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6mn-yEnmEu4O"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "# Example news article text\n",
        "article_text = \"\"\"\n",
        "The Los Angeles Lakers defeated the Brooklyn Nets 107-96 on Saturday. LeBron James led the Lakers with 32 points and 8 rebounds. \n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of words feature\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "tokens = word_tokenize(article_text.lower())\n",
        "filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "word_freq = nltk.FreqDist(filtered_words)\n",
        "bag_of_words_feature = dict(word_freq)\n",
        "\n",
        "print(bag_of_words_feature)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCAmDdUgF69O",
        "outputId": "907d9b5c-f359-4449-c7c6-35482657133a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'los': 1, 'angeles': 1, 'lakers': 2, 'defeated': 1, 'brooklyn': 1, 'nets': 1, '107-96': 1, 'saturday': 1, '.': 2, 'lebron': 1, 'james': 1, 'led': 1, '32': 1, 'points': 1, '8': 1, 'rebounds': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named entities feature\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "entities = ne_chunk(pos_tag(word_tokenize(article_text)))\n",
        "named_entities = [chunk for chunk in entities if hasattr(chunk, 'label')]\n",
        "\n",
        "named_entities_feature = {}\n",
        "for chunk in named_entities:\n",
        "    entity = ' '.join(c[0] for c in chunk.leaves())\n",
        "    named_entities_feature[entity] = named_entities_feature.get(entity, 0) + 1\n",
        "\n",
        "print(named_entities_feature)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I02JG-8XF-RR",
        "outputId": "bff57792-a9e6-462d-c1fa-7d9b442075b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Los Angeles': 1, 'Brooklyn Nets': 1, 'LeBron James': 1, 'Lakers': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part-of-speech tags feature\n",
        "pos_tags = pos_tag(word_tokenize(article_text))\n",
        "\n",
        "pos_tags_feature = {}\n",
        "for tag in pos_tags:\n",
        "    pos_tags_feature[tag[1]] = pos_tags_feature.get(tag[1], 0) + 1\n",
        "\n",
        "print(pos_tags_feature)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGZPCZUyGBrV",
        "outputId": "0745b7a9-3925-4838-e260-3d0890e980b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'DT': 3, 'NNP': 9, 'VBD': 2, 'JJ': 1, 'IN': 2, '.': 2, 'CD': 2, 'NNS': 2, 'CC': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment feature\n",
        "from textblob import TextBlob\n",
        "\n",
        "sentiment = TextBlob(article_text).sentiment\n",
        "sentiment_feature = {'polarity': sentiment.polarity, 'subjectivity': sentiment.subjectivity}\n",
        "\n",
        "print(sentiment_feature)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw5aIY7bGDI0",
        "outputId": "9fdf0798-d97d-4051-b5a6-98bef32fe3ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'polarity': 0.0, 'subjectivity': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-BoN6i0GolQ",
        "outputId": "0d670c0e-c0e8-40cb-8d49-bda786acb61e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.13.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.13.2 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Readability feature\n",
        "from textstat import flesch_reading_ease\n",
        "\n",
        "readability = flesch_reading_ease(article_text)\n",
        "readability_feature = {'readability': readability}\n",
        "\n",
        "print(readability_feature)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtSKddReGEm-",
        "outputId": "10ee2692-e027-4a45-fd4d-a3d533a35be4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'readability': 68.77}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1B_DTkqEu4P"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUo5WIjbEu4P",
        "outputId": "ffdb738f-f402-4799-93f4-bad32010aff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Features:\n",
            "lakers 6.0\n",
            "defeated 3.0\n",
            "iphone 3.0\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Example news article texts\n",
        "article_texts = [\n",
        "    \"The Los Angeles Lakers defeated the Brooklyn Nets 107-96 on Saturday. LeBron James led the Lakers with 32 points and 8 rebounds.\",\n",
        "    \"The United States imposed new sanctions on Russia on Friday over its alleged interference in the 2020 U.S. presidential election.\",\n",
        "    \"The new iPhone 14 is expected to have a larger screen and better camera than its predecessor.\",\n",
        "    \"Scientists have discovered a new species of dinosaur in Argentina that had wings and could fly.\"\n",
        "]\n",
        "\n",
        "# Target categories for each article\n",
        "categories = [\"Sports\", \"Politics\", \"Technology\", \"Science\"]\n",
        "\n",
        "# Combine all the texts into a single list\n",
        "all_texts = []\n",
        "for i in range(len(article_texts)):\n",
        "    all_texts.append(article_texts[i] + \" \" + categories[i])\n",
        "\n",
        "# Create a bag of words matrix\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
        "X = vectorizer.fit_transform(all_texts)\n",
        "y = np.array(categories)\n",
        "\n",
        "# Select the top 3 features using the Chi-squared test\n",
        "selector = SelectKBest(chi2, k=3)\n",
        "selector.fit(X, y)\n",
        "\n",
        "# Get the scores and feature names\n",
        "scores = selector.scores_\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Sort the features by score in descending order\n",
        "sorted_indices = np.argsort(scores)[::-1]\n",
        "sorted_features = feature_names[sorted_indices]\n",
        "\n",
        "# Print the top 3 features and their scores\n",
        "print(\"Top 3 Features:\")\n",
        "for i in range(3):\n",
        "    print(sorted_features[i], scores[sorted_indices[i]])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzSTz-4LEu4R"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYUz0KssJB73",
        "outputId": "c139b4a0-1d96-44db-b7d0-d784ecf378b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc_F1JzHEu4S",
        "outputId": "ee923802-517f-4db4-f21b-8f18c0ddbc53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar texts:\n",
            "The Los Angeles Lakers defeated the Brooklyn Nets 107-96 on Saturday. LeBron James led the Lakers with 32 points and 8 rebounds.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Example news article texts\n",
        "article_texts = [\"The Los Angeles Lakers defeated the Brooklyn Nets 107-96 on Saturday. LeBron James led the Lakers with 32 points and 8 rebounds.\"]\n",
        "# Query\n",
        "query = \"LeBron James scored 32 points in Lakers' win over the Nets\"\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the query and each article text\n",
        "query_tokens = tokenizer.tokenize(query)\n",
        "article_tokens = [tokenizer.tokenize(text) for text in article_texts]\n",
        "\n",
        "# Convert the tokens to IDs\n",
        "query_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n",
        "article_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in article_tokens]\n",
        "\n",
        "# Pad the IDs to the maximum sequence length\n",
        "max_length = 64\n",
        "query_ids = query_ids[:max_length] + [0] * (max_length - len(query_ids))\n",
        "article_ids = [ids[:max_length] + [0] * (max_length - len(ids)) for ids in article_ids]\n",
        "\n",
        "# Convert the IDs to PyTorch tensors\n",
        "query_ids = torch.tensor(query_ids).unsqueeze(0)\n",
        "article_ids = torch.tensor(article_ids)\n",
        "\n",
        "# Generate BERT embeddings for the query and each article text\n",
        "with torch.no_grad():\n",
        "    query_embedding = model(query_ids)[0][:, 0, :].numpy()\n",
        "    article_embeddings = model(article_ids)[0][:, 0, :].numpy()\n",
        "\n",
        "# Calculate the cosine similarity between the query and each article text\n",
        "similarities = cosine_similarity(query_embedding, article_embeddings).squeeze()\n",
        "\n",
        "# Sort the similarities in descending order and get the corresponding indices\n",
        "indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "# Print the texts in descending order of similarity to the query\n",
        "print(\"Most similar texts:\")\n",
        "for i in indices:\n",
        "    print(article_texts[i])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}